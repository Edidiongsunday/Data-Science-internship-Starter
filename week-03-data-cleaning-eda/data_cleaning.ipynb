{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73f7834",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook\n",
    "\n",
    "## Week 3: Load, Inspect, and Clean Your Data\n",
    "\n",
    "In this notebook, we'll take a raw dataset and transform it into a clean, ready-to-analyze format.\n",
    "\n",
    "### Steps:\n",
    "1. Load the data\n",
    "2. Inspect its structure and quality\n",
    "3. Handle missing values\n",
    "4. Remove duplicates\n",
    "5. Fix data types\n",
    "6. Handle outliers\n",
    "7. Export clean data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4131df",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd176c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2756f",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "First, we need to load a dataset. You can use:\n",
    "- A CSV file from your computer\n",
    "- A CSV from Kaggle\n",
    "- A built-in dataset like Iris or Titanic\n",
    "\n",
    "For this example, we'll use a sample dataset. Replace this with your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9be659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your dataset\n",
    "# Option 1: From a CSV file on your computer\n",
    "# df = pd.read_csv('../datasets/your_dataset.csv')\n",
    "\n",
    "# Option 2: Create a sample dataset for practice\n",
    "df = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Age': [25, 30, np.nan, 28, 35, 29, 26, np.nan],\n",
    "    'Salary': [50000, 60000, 55000, 65000, 75000, 70000, 52000, 68000],\n",
    "    'Department': ['Sales', 'IT', 'HR', 'Sales', 'IT', 'HR', 'IT', 'Sales'],\n",
    "    'Years': [2, 5, 3, 4, 7, 6, 2, 4]\n",
    "})\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789974a",
   "metadata": {},
   "source": [
    "## Step 2: Inspect the Data\n",
    "\n",
    "Before cleaning, understand what you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "print(\"Data Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values Percentage:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50272f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Total duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check for duplicates based on specific columns\n",
    "print(f\"\\nDuplicate IDs: {df.duplicated(subset=['ID']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776cbc7",
   "metadata": {},
   "source": [
    "## Step 3: Handle Missing Values\n",
    "\n",
    "Different strategies for different situations:\n",
    "- **Drop**: Remove rows with missing values (use if <5% missing)\n",
    "- **Fill with mean/median**: Preserve data size (numerical columns)\n",
    "- **Fill with mode**: For categorical columns\n",
    "- **Forward/Backward fill**: For time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handle missing values in 'Age'\n",
    "print(\"Original Age column:\")\n",
    "print(df['Age'])\n",
    "\n",
    "# Option 1: Fill with median (good for numerical data)\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "\n",
    "print(\"\\nAge after filling with median:\")\n",
    "print(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1720c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify missing values are gone\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021ab9e",
   "metadata": {},
   "source": [
    "## Step 4: Remove Duplicates\n",
    "\n",
    "Identical rows should typically be removed (unless they represent real duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ace03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for complete duplicates\n",
    "print(f\"Duplicate rows before removal: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "\n",
    "print(f\"Duplicate rows after removal: {df_clean.duplicated().sum()}\")\n",
    "print(f\"\\nRows removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89a40b",
   "metadata": {},
   "source": [
    "## Step 5: Fix Data Types\n",
    "\n",
    "Ensure each column has the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current data types:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert types if needed\n",
    "# df_clean['ID'] = df_clean['ID'].astype(int)\n",
    "# df_clean['Age'] = df_clean['Age'].astype(int)\n",
    "# df_clean['Salary'] = df_clean['Salary'].astype(float)\n",
    "\n",
    "print(\"\\nData types are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d50dac",
   "metadata": {},
   "source": [
    "## Step 6: Handle Outliers\n",
    "\n",
    "Outliers might be errors or interesting insights. Investigate before removing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method for numerical columns\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Check for outliers in Salary\n",
    "outliers = detect_outliers_iqr(df_clean, 'Salary')\n",
    "print(f\"Outliers in Salary column:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb78e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decide whether to remove, cap, or keep outliers\n",
    "# For now, we'll keep them as they might be legitimate high earners!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd24f3",
   "metadata": {},
   "source": [
    "## Step 7: Final Data Quality Check & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nShape: {df_clean.shape}\")\n",
    "print(f\"Memory usage: {df_clean.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nMissing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "print(f\"\\nData types correct: Yes\")\n",
    "print(f\"\\nFirst few rows of cleaned data:\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e097148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned data\n",
    "output_path = '../datasets/cleaned_data.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5677f8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Data loaded and inspected\n",
    "✅ Missing values handled\n",
    "✅ Duplicates removed\n",
    "✅ Data types corrected\n",
    "✅ Outliers investigated\n",
    "✅ Clean data exported\n",
    "\n",
    "**Next:** Use this cleaned data in the `eda.ipynb` notebook for analysis!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
